{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOVt4Cz5GQ54gHv5QvH/fVD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"lEtJnLWoBKJF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQAqwWVF2VI6"},"outputs":[],"source":["%%bash\n","# Install deps from \n","# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n","\n","apt-get update\n","\n","\n","apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n","nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n","libopenal-dev timidity libwildmidi-dev unzip\n","\n","# Boost libraries\n","apt-get install libboost-all-dev\n"]},{"cell_type":"code","source":["!pip install vizdoom --quiet \n","!pip install ray  --quiet\n","!pip install ray['rllib'] --quiet\n","!pip install Ipython --upgrade --quiet"],"metadata":{"id":"hCF837q_2_UK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os \n","from google.colab import drive\n","import sys\n","\n","\n","#need this to load vizdoom module \n","system_path = '/content/drive/MyDrive/GitHub/INM363-Project'\n","drive.mount('/content/drive')\n","sys.path.append(system_path)\n","\n","system_path = '/content/drive/MyDrive/GitHub/INM363-Project/src' \n","sys.path.append(system_path)\n","\n","#need this to use gpu on ray \n","os.environ['PYTHONPATH'] = '/content/drive/MyDrive/GitHub/INM363-Project' \n","os.environ['PYTHONPATH']"],"metadata":{"id":"ywVOUiIS2h-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from src.vizdoom_gym.envs.VizDoomEnv import VizdoomEnv\n","from src.vizdoom_gym.envs.VizDoomEnv_def import VizDoomVeryDenseReward"],"metadata":{"id":"hy6x2ebM2oHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ray.tune.registry import register_env\n","import gym\n","import os\n","import ray\n","import ray.rllib.agents.ppo as ppo\n","from ray.rllib.algorithms.callbacks import RE3UpdateCallbacks\n","import shutil\n","import torch\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math \n","from fnmatch import fnmatch \n","import numpy as np\n","\n","sns.set()"],"metadata":{"id":"c1pU7RiQ2pzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device: \", device, \"\\n\")\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"r0179tyn2r0r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialize Ray"],"metadata":{"id":"AcoaqwWs3Iu2"}},{"cell_type":"code","source":["\n","ray.shutdown()\n","print(\"Shutdown ray\")\n","# start Ray -- add `local_mode=True` here for debugging\n","ray.init(ignore_reinit_error=True,  num_cpus =1, num_gpus = 1) #local_mode=True,\n","\n","print(\"Initialized ray\")\n","\n","# register the custom environment\n","select_env = \"VizDoomVeryDenseReward-v0\"\n","\n","register_env(select_env, lambda config: VizDoomVeryDenseReward())\n","\n","print(\"registered environment\")"],"metadata":{"id":"uU7im0zk2yIk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PPO"],"metadata":{"id":"5cLfqGQV3Lx_"}},{"cell_type":"code","source":["# configure the environment and create agent\n","config = ppo.DEFAULT_CONFIG.copy()\n","config[\"log_level\"] = \"WARN\"\n","config[\"num_workers\"] = 0\n","config[\"framework\"] = \"torch\"\n","config[\"model\"] = {\"dim\": 42, \n","                   \"grayscale\": True,\n","                   }\n","config[\"num_gpus\"] = 1\n","config[\"preprocessor_pref\"] = \"rllib\"\n","config['batch_mode'] = 'complete_episodes'\n","\n","#changing this for evaluation time \n","config['explore'] = True \n","config['in_evaluation'] = True\n","\n","agent = ppo.PPOTrainer(config, env=select_env)\n","\n","print(\"created agent\")"],"metadata":{"id":"gMYLkSEx3TBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# examine the trained policy\n","#policy = agent.get_policy()\n","#model = policy.model\n","\n","#create environment \n","env = gym.make('VizDoomVeryDenseReward-v0')"],"metadata":{"id":"Cx5wKkuC3hbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/ppo/easy_dense\" \n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","\n","ppo_chkpt_mean_rewards = np.array([]) \n","ppo_chkpt_std_rewards = np.array([]) \n","ppo_chkpt_mean_steps = np.array([]) \n","ppo_chkpt_std_steps = np.array([]) \n","\n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10\n","  total_reward = []\n","  total_steps = [] \n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    eps_reward = 0\n","    eps_steps = 0 \n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          eps_reward += reward\n","\n","          #if episode ends early\n","          if done == 1:\n","              total_reward.append(eps_reward)\n","              total_steps.append(step+1)\n","              state = env.reset()\n","              break\n","\n","    #if episode timeout\n","    if done == 0:\n","      total_reward.append(eps_reward)\n","      total_steps.append(n_step)\n","\n","  #get mean and std over all episodes\n","  ppo_chkpt_mean_rewards = np.append(ppo_chkpt_mean_rewards, np.mean(total_reward) )\n","  ppo_chkpt_std_rewards = np.append(ppo_chkpt_std_rewards, np.std(total_reward))\n","  ppo_chkpt_mean_steps = np.append(ppo_chkpt_mean_steps, np.mean(total_steps))\n","  ppo_chkpt_std_steps = np.append(ppo_chkpt_std_steps, np.std(total_steps))\n","\n"],"metadata":{"id":"hZ6JjAX33qym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_pth = chkpt_root + '/' + 'result.csv'\n","ppo_df = pd.read_csv(results_pth) \n","ppo_df['eps_cumsum'] = ppo_df['episodes_this_iter'].cumsum()\n","ppo_df = ppo_df[['checkpoint', 'eps_cumsum']]\n","ppo_df.checkpoint = ppo_df['checkpoint'].astype(int)\n","ppo_df.head()\n","\n","ppo_cum_eps = [] \n","for chkpt in checkpoints:\n","  chkpt_int = int(chkpt.split('_')[-1])\n","  eps_num = ppo_df[~(ppo_df['eps_cumsum'].where(ppo_df['checkpoint'] == chkpt_int)).isna()].eps_cumsum.values[0]\n","  ppo_cum_eps.append(eps_num)"],"metadata":{"id":"RVL9w1Xk4yvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Total episodes: {ppo_cum_eps}\")\n","print(f\"Mean reward: {ppo_chkpt_mean_rewards}\")\n","print(f\"Std reward: {ppo_chkpt_std_rewards}\")\n","print(f\"Mean steps: {ppo_chkpt_mean_steps}\")\n","print(f\"Std steps: {ppo_chkpt_std_steps}\")\n","\n","#check length of lists are the same \n","assert len(ppo_cum_eps) == len(ppo_chkpt_mean_rewards)\n","assert len(ppo_cum_eps) == len(ppo_chkpt_mean_steps)\n","\n"],"metadata":{"id":"OFNRtZIn5sk1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PPO avg reward"],"metadata":{"id":"ZAYAgXq5ed5W"}},{"cell_type":"code","source":["\n","\n","# https://stackoverflow.com/questions/61368805/how-to-plot-shaded-error-bands-with-seaborn\n","\n","plt.plot(ppo_cum_eps, ppo_chkpt_mean_rewards, 'b-', label='PPO')\n","plt.fill_between(ppo_cum_eps, ppo_chkpt_mean_rewards - ppo_chkpt_std_rewards, ppo_chkpt_mean_rewards + ppo_chkpt_std_rewards, color='b', alpha=0.2) \n","\n","\n","plt.legend(title='legend')\n","plt.title(\"ICM avg reward\") \n","plt.show()\n"],"metadata":{"id":"5q7yZCyZGukb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PPO avg steps"],"metadata":{"id":"FwY_wnldfAZZ"}},{"cell_type":"code","source":["\n","\n","# https://stackoverflow.com/questions/61368805/how-to-plot-shaded-error-bands-with-seaborn\n","\n","plt.plot(ppo_cum_eps, ppo_chkpt_mean_steps, 'b-', label='PPO')\n","plt.fill_between(ppo_cum_eps, (ppo_chkpt_mean_steps - ppo_chkpt_std_steps), (ppo_chkpt_mean_steps + ppo_chkpt_std_steps), color='b', alpha=0.2) \n","\n","plt.legend(title='legend')\n","plt.title(\"PPO avg steps\") \n","plt.show()\n"],"metadata":{"id":"_uhBYlIueho5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ICM "],"metadata":{"id":"9-XwCWBmBO-H"}},{"cell_type":"code","source":["# configure the environment and create agent\n","config = ppo.DEFAULT_CONFIG.copy()\n","config[\"log_level\"] = \"WARN\"\n","#config[\"num_workers\"] = 1\n","config[\"framework\"] = \"torch\"\n","config[\"model\"] = {\"dim\": 42, \n","                   \"grayscale\": True,\n","                   }\n","config[\"num_gpus\"] = 1\n","config[\"preprocessor_pref\"] = \"rllib\"\n","config['batch_mode'] = 'complete_episodes'\n","\n","#changing this for evaluation time \n","config['explore'] = True\n","config['in_evaluation'] = True\n","\n","config[\"num_workers\"] = 0 \n","\n","\n","config[\"exploration_config\"] = {\n","    \"type\": \"Curiosity\",  # <- Use the Curiosity module for exploring.\n","    \"eta\": 0.01, #0.001,  # Weight for intrinsic rewards before being added to extrinsic ones.\n","    \"lr\": 0.001,  # Learning rate of the curiosity (ICM) module.\n","    \"feature_dim\": 288,  # Dimensionality of the generated feature vectors.\n","    # Setup of the feature net (used to encode observations into feature (latent) vectors).\n","    \"feature_net_config\": {\n","        \"fcnet_hiddens\": [],\n","        \"fcnet_activation\": \"relu\",\n","    },\n","    \"inverse_net_hiddens\": [256],  # Hidden layers of the \"inverse\" model.\n","    \"inverse_net_activation\": \"relu\",  # Activation of the \"inverse\" model.\n","    \"forward_net_hiddens\": [256],  # Hidden layers of the \"forward\" model.\n","    \"forward_net_activation\": \"relu\",  # Activation of the \"forward\" model.\n","    \"beta\": 0.2,  # Weight for the \"forward\" loss (beta) over the \"inverse\" loss (1.0 - beta).\n","    # Specify, which exploration sub-type to use (usually, the algo's \"default\"\n","    # exploration, e.g. EpsilonGreedy for DQN, StochasticSampling for PG/SAC).\n","   \n","    \"sub_exploration\": {\n","#        \"type\": \"EpsilonGreedy\",\n","#        \"initial_epsilon\": 1.0, #0.01,\n","#        \"final_epsilon\": 0.01, #0.001\n","        \"type\": \"StochasticSampling\",\n","        \n","    }\n","}\n","config[\"vf_clip_param\"] = 400\n","agent = ppo.PPOTrainer(config, env=select_env)\n","print(\"created agent\")"],"metadata":{"id":"DYN7_qcfBSh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create environment \n","env = gym.make('VizDoomVeryDenseReward-v0')"],"metadata":{"id":"4zNcb6wEBph3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/icm/easy_dense_no_tpenalty\" \n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","#chkpt_mean_rewards = [] \n","#chkpt_me\n","\n","icm_chkpt_mean_rewards = np.array([]) \n","icm_chkpt_std_rewards = np.array([]) \n","icm_chkpt_mean_steps = np.array([]) \n","icm_chkpt_std_steps = np.array([]) \n","\n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10\n","  total_reward = []\n","  total_steps = [] \n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    eps_reward = 0\n","    eps_steps = 0 \n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          eps_reward += reward\n","\n","          #if episode ends early\n","          if done == 1:\n","              total_reward.append(eps_reward)\n","              total_steps.append(step+1)\n","              state = env.reset()\n","              break\n","\n","    #if episode timeout\n","    if done == 0:\n","      total_reward.append(eps_reward)\n","      total_steps.append(n_step)\n","\n","  #get mean and std over all episodes\n","  icm_chkpt_mean_rewards = np.append(icm_chkpt_mean_rewards, np.mean(total_reward) )\n","  icm_chkpt_std_rewards = np.append(icm_chkpt_std_rewards, np.std(total_reward))\n","  icm_chkpt_mean_steps = np.append(icm_chkpt_mean_steps, np.mean(total_steps))\n","  icm_chkpt_std_steps = np.append(icm_chkpt_std_steps, np.std(total_steps))\n","\n"],"metadata":{"id":"KhKF1OfHCfyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_pth = chkpt_root + '/' + 'result.csv'\n","icm_df = pd.read_csv(results_pth) \n","icm_df['eps_cumsum'] = icm_df['episodes_this_iter'].cumsum()\n","icm_df = ppo_df[['checkpoint', 'eps_cumsum']]\n","icm_df.checkpoint = icm_df['checkpoint'].astype(int)\n","icm_df.head()"],"metadata":{"id":"u_raWHVFFKrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["icm_cum_eps = [] \n","for chkpt in checkpoints:\n","  chkpt_int = int(chkpt.split('_')[-1])\n","  eps_num = icm_df[~(icm_df['eps_cumsum'].where(icm_df['checkpoint'] == chkpt_int)).isna()].eps_cumsum.values[0]\n","  icm_cum_eps.append(eps_num)\n","\n","print(f\"Total episodes: {icm_cum_eps}\")\n","print(f\"Mean reward: {icm_chkpt_mean_rewards}\")\n","print(f\"Std reward: {icm_chkpt_std_rewards}\")\n","print(f\"Mean steps: {icm_chkpt_mean_steps}\")\n","print(f\"Std steps: {icm_chkpt_std_steps}\")\n","\n","#check length of lists are the same \n","assert len(icm_cum_eps) == len(icm_chkpt_mean_rewards)\n","assert len(icm_cum_eps) == len(icm_chkpt_mean_steps)"],"metadata":{"id":"Es5eogKMC9yt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ICM avg reward"],"metadata":{"id":"urlcI5W3PVso"}},{"cell_type":"code","source":["\n","\n","# https://stackoverflow.com/questions/61368805/how-to-plot-shaded-error-bands-with-seaborn\n","\n","plt.plot(icm_cum_eps, icm_chkpt_mean_rewards, 'b-', label='icm_avg_reward')\n","plt.fill_between(icm_cum_eps, (icm_chkpt_mean_rewards - icm_chkpt_std_rewards), (icm_chkpt_mean_rewards + icm_chkpt_std_rewards), color='b', alpha=0.2) \n","\n","plt.legend(title='legend')\n","plt.title(\"ICM avg reward\") \n","plt.show()\n","\n"],"metadata":{"id":"MitIhHcrDBGm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ICM avg steps"],"metadata":{"id":"VmUcC4jBPYkp"}},{"cell_type":"code","source":["\n","\n","# https://stackoverflow.com/questions/61368805/how-to-plot-shaded-error-bands-with-seaborn\n","\n","plt.plot(icm_cum_eps, icm_chkpt_mean_steps, 'b-', label='icm_avg_steps')\n","plt.fill_between(icm_cum_eps, (icm_chkpt_mean_steps - icm_chkpt_std_steps), (icm_chkpt_mean_steps + icm_chkpt_std_steps), color='b', alpha=0.2) \n","\n","plt.legend(title='legend')\n","plt.title(\"ICM avg steps\") \n","plt.show()\n","\n"],"metadata":{"id":"0WolCEMIDGVw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RE3"],"metadata":{"id":"lI2qt8HIBQXH"}},{"cell_type":"code","source":["class RE3Callbacks(RE3UpdateCallbacks, config[\"callbacks\"]):\n","  pass"],"metadata":{"id":"6pKp-588-G-q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure the environment and create agent\n","config = ppo.DEFAULT_CONFIG.copy()\n","config[\"log_level\"] = \"WARN\"\n","config[\"model\"] = {\"dim\": 42, \n","                   \"grayscale\": True,\n","                   }\n","config[\"num_gpus\"] = 1\n","config[\"num_workers\"] = 0 #check why this is set to 0!  \n","config[\"preprocessor_pref\"] = \"rllib\"\n","config['explore'] = True \n","config['in_evaluation'] = True\n","config[\"framework\"] = \"tf\"\n","config[\"seed\"] = 12345\n","config[\"callbacks\"] = RE3Callbacks\n","config[\"exploration_config\"] = {\n","    \"type\": \"RE3\",\n","     \"embeds_dim\": 128,\n","     \"rho\": 0.1, # Beta decay factor, used for on-policy algorithm.\n","     \"k_nn\": 7, # Number of neighbours to set for K-NN entropy estimation.\n","     \"encoder_net_config\": {\n","         \"fcnet_hiddens\": [],\n","         \"fcnet_activation\": \"relu\",\n","     },\n","     # `reward = r + beta * intrinsic_reward`\n","     \"beta\": 0.2,\n","     # Schedule to use for beta decay, one of constant\" or \"linear_decay\".\n","     \"beta_schedule\": 'constant',\n","     # Specify, which exploration sub-type to use (usually, the algo's \"default\"\n","     # exploration, e.g. EpsilonGreedy for DQN, StochasticSampling for PG/SAC).\n","     \"sub_exploration\": {\n","         \"type\": \"StochasticSampling\",\n","     }\n","}\n","agent = ppo.PPOTrainer(config, env=select_env)\n","print(\"created agent\")"],"metadata":{"id":"hPktRgwyQiEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make('VizDoomVeryDenseReward-v0')"],"metadata":{"id":"KCzdE7ZMQ8co"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/re3/easy_dense\" \n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","\n","re3_chkpt_mean_rewards = np.array([]) \n","re3_chkpt_std_rewards = np.array([]) \n","re3_chkpt_mean_steps = np.array([]) \n","re3_chkpt_std_steps = np.array([]) \n","\n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10\n","  total_reward = []\n","  total_steps = [] \n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    eps_reward = 0\n","    eps_steps = 0 \n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          eps_reward += reward\n","\n","          #if episode ends early\n","          if done == 1:\n","              total_reward.append(eps_reward)\n","              total_steps.append(step+1)\n","              state = env.reset()\n","              break\n","\n","    #if episode timeout\n","    if done == 0:\n","      total_reward.append(eps_reward)\n","      total_steps.append(n_step)\n","\n","  #get mean and std over all episodes\n","  re3_chkpt_mean_rewards = np.append(re3_chkpt_mean_rewards, np.mean(total_reward) )\n","  re3_chkpt_std_rewards = np.append(re3_chkpt_std_rewards, np.std(total_reward))\n","  re3_chkpt_mean_steps = np.append(re3_chkpt_mean_steps, np.mean(total_steps))\n","  re3_chkpt_std_steps = np.append(re3_chkpt_std_steps, np.std(total_steps))\n","\n"],"metadata":{"id":"dE8wrZe6RT79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_pth = chkpt_root + '/' + 'result.csv'\n","re3_df = pd.read_csv(results_pth) \n","re3_df['eps_cumsum'] = re3_df['episodes_this_iter'].cumsum()\n","re3_df = re3_df[['checkpoint', 'eps_cumsum']]\n","re3_df.checkpoint = re3_df['checkpoint'].astype(int)\n","re3_df.head()"],"metadata":{"id":"kA8Tkp0nRuYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["re3_cum_eps = [] \n","for chkpt in checkpoints:\n","  chkpt_int = int(chkpt.split('_')[-1])\n","  eps_num = re3_df[~(re3_df['eps_cumsum'].where(re3_df['checkpoint'] == chkpt_int)).isna()].eps_cumsum.values[0]\n","  re3_cum_eps.append(eps_num)\n","\n","print(f\"Total episodes: {re3_cum_eps}\")\n","print(f\"Mean reward: {re3_chkpt_mean_rewards}\")\n","print(f\"Std reward: {re3_chkpt_std_rewards}\")\n","print(f\"Mean steps: {re3_chkpt_mean_steps}\")\n","print(f\"Std steps: {re3_chkpt_std_steps}\")\n","\n","#check length of lists are the same \n","assert len(re3_cum_eps) == len(re3_chkpt_mean_rewards)\n","assert len(re3_cum_eps) == len(re3_chkpt_mean_steps)"],"metadata":{"id":"CO9OJG-rRx0I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RE3 avg reward"],"metadata":{"id":"sEwK3Fj5TgVa"}},{"cell_type":"code","source":["# https://stackoverflow.com/questions/61368805/how-to-plot-shaded-error-bands-with-seaborn\n","\n","plt.plot(re3_cum_eps, re3_chkpt_mean_rewards, 'b-', label='re3_avg_reward')\n","plt.fill_between(re3_cum_eps, (re3_chkpt_mean_rewards - re3_chkpt_std_rewards), (re3_chkpt_mean_rewards + re3_chkpt_std_rewards), color='b', alpha=0.2) \n","\n","plt.legend(title='legend')\n","plt.title(\"RE3 avg reward\") \n","plt.show()\n","\n"],"metadata":{"id":"3OCT243hTSjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RE3 avg steps"],"metadata":{"id":"naSxSkWwTyac"}},{"cell_type":"code","source":["\n","\n","# https://stackoverflow.com/questions/61368805/how-to-plot-shaded-error-bands-with-seaborn\n","\n","plt.plot(re3_cum_eps, re3_chkpt_mean_steps, 'r-', label='re3_avg_steps')\n","plt.fill_between(re3_cum_eps, (re3_chkpt_mean_steps - re3_chkpt_std_steps), (re3_chkpt_mean_steps + re3_chkpt_std_steps), \n","                 color='r', alpha=0.2) \n","\n","plt.legend(title='legend')\n","plt.title(\"RE3 avg steps\") \n","plt.show()\n","\n"],"metadata":{"id":"S0yrgJIgTuor"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# All graphs"],"metadata":{"id":"VcGfOVC0UnZH"}},{"cell_type":"markdown","source":["## Average Reward"],"metadata":{"id":"d5KcRdDxUr7T"}},{"cell_type":"code","source":["sns.set_style(\"whitegrid\", {'axes.grid' : False})\n","\n","#PPO \n","plt.plot(ppo_cum_eps, ppo_chkpt_mean_rewards, 'k-', label='PPO')\n","plt.fill_between(ppo_cum_eps, ppo_chkpt_mean_rewards - ppo_chkpt_std_rewards, ppo_chkpt_mean_rewards + ppo_chkpt_std_rewards, color='k', alpha=0.2) \n","\n","#ICM\n","plt.plot(icm_cum_eps, icm_chkpt_mean_rewards, 'r-', label='PPO+ICM')\n","plt.fill_between(icm_cum_eps, (icm_chkpt_mean_rewards - icm_chkpt_std_rewards), (icm_chkpt_mean_rewards + icm_chkpt_std_rewards), color='r', alpha=0.2) \n","#RE3\n","plt.plot(re3_cum_eps[:6], re3_chkpt_mean_rewards[:6], 'b-', label='PPO+RE3')\n","plt.fill_between(re3_cum_eps[:6], (re3_chkpt_mean_rewards - re3_chkpt_std_rewards)[:6], (re3_chkpt_mean_rewards + re3_chkpt_std_rewards)[:6], color='b', alpha=0.2) \n","\n","\n","plt.legend()\n","plt.xlabel('Number of Episodes') \n","plt.ylabel('Total Rewards') \n","plt.title(\"Average Rewards for Experiment 0\") \n","plt.show()\n"],"metadata":{"id":"-YDQreaNUE0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Average Steps"],"metadata":{"id":"raG4InxKgJt_"}},{"cell_type":"code","source":["\n","#PPO\n","plt.plot(ppo_cum_eps, ppo_chkpt_mean_steps, 'k-', label='PPO')\n","plt.fill_between(ppo_cum_eps, (ppo_chkpt_mean_steps - ppo_chkpt_std_steps), (ppo_chkpt_mean_steps + ppo_chkpt_std_steps), color='k', alpha=0.2) \n","\n","#ICM\n","plt.plot(icm_cum_eps, icm_chkpt_mean_steps, 'r-', label='PPO+ICM')\n","plt.fill_between(icm_cum_eps, (icm_chkpt_mean_steps - icm_chkpt_std_steps), (icm_chkpt_mean_steps + icm_chkpt_std_steps), color='r', alpha=0.2) \n","\n","#RE3\n","plt.plot(re3_cum_eps[:6], re3_chkpt_mean_steps[:6], 'b-', label='PPO+RE3')\n","plt.fill_between(re3_cum_eps[:6], (re3_chkpt_mean_steps - re3_chkpt_std_steps)[:6], (re3_chkpt_mean_steps + re3_chkpt_std_steps)[:6], color='b', alpha=0.2) \n","\n","plt.legend()\n","plt.xlabel('Number of Episodes') \n","plt.ylabel('Episode Length') \n","plt.title(\"Average Episode Length for Experiment 0\") \n","plt.show()"],"metadata":{"id":"uORbBy6HVQik"},"execution_count":null,"outputs":[]}]}