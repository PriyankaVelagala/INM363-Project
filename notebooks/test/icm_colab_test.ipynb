{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMYErEJuuMAfzSOvUqR6BHa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup \n","\n"],"metadata":{"id":"z_6MsycFCZFj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdHKp83oCNYP"},"outputs":[],"source":["%%bash\n","# Install deps from \n","# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n","\n","apt-get update\n","\n","\n","apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n","nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n","libopenal-dev timidity libwildmidi-dev unzip\n","\n","# Boost libraries\n","apt-get install libboost-all-dev"]},{"cell_type":"code","source":["!pip install vizdoom\n","!pip install ray \n","!pip install ray['rllib']\n","!pip install Ipython --upgrade\n"],"metadata":{"id":"WF4YrHzsCdQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os \n","from google.colab import drive\n","import sys\n","\n","\n","#need this to load vizdoom module \n","system_path = '/content/drive/MyDrive/GitHub/INM363-Project'\n","drive.mount('/content/drive')\n","sys.path.append(system_path)\n","\n","system_path = '/content/drive/MyDrive/GitHub/INM363-Project/src' \n","sys.path.append(system_path)\n","\n","#need this to use gpu on ray \n","os.environ['PYTHONPATH'] = '/content/drive/MyDrive/GitHub/INM363-Project' \n","os.environ['PYTHONPATH']"],"metadata":{"id":"RFtsvfimEaWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WldQjKYwGDNi"},"outputs":[],"source":["from src.vizdoom_gym.envs.VizDoomEnv import VizdoomEnv\n","from src.vizdoom_gym.envs.VizDoomEnv_def import VizDoomVeryDenseReward"]},{"cell_type":"code","source":["from ray.tune.registry import register_env\n","import gym\n","import ray\n","import ray.rllib.agents.ppo as ppo\n","import shutil\n","import torch"],"metadata":{"id":"gFV-M26PCh3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device: \", device, \"\\n\")\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"AImUdHmoCmsC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"H_T2Tth3CX2h"}},{"cell_type":"markdown","source":["# Create and restore agent from checkpoint\n","\n"],"metadata":{"id":"RKT1APBrCpwK"}},{"cell_type":"code","source":["\n","ray.shutdown()\n","print(\"Shutdown ray\")\n","# start Ray -- add `local_mode=True` here for debugging\n","ray.init(ignore_reinit_error=True,  num_cpus =2, num_gpus = 1) #local_mode=True,\n","\n","print(\"Initialized ray\")\n","\n","# register the custom environment\n","select_env = \"VizDoomVeryDenseReward-v0\"\n","\n","register_env(select_env, lambda config: VizDoomVeryDenseReward())\n","\n","print(\"registered environment\")\n"],"metadata":{"id":"nA54sQqEC5FG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure the environment and create agent\n","config = ppo.DEFAULT_CONFIG.copy()\n","config[\"log_level\"] = \"WARN\"\n","#config[\"num_workers\"] = 1\n","config[\"framework\"] = \"torch\"\n","config[\"model\"] = {\"dim\": 42, \n","                   \"grayscale\": True,\n","                   }\n","config[\"num_gpus\"] = 1\n","config[\"preprocessor_pref\"] = \"rllib\"\n","config['batch_mode'] = 'complete_episodes'\n","\n","#changing this for evaluation time \n","config['explore'] = True #for ICM and RE3 might need to be left true to use the exploration module \n","config['in_evaluation'] = True\n","\n","\n","#config[\"horizon\"] = 50\n","#agent = ppo.PPOTrainer(config, env=select_env)\n","#print(\"created agent\")"],"metadata":{"id":"Y47Q-1FIC6t6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#activating curiosity as the exploration class : https://docs.ray.io/en/latest/rllib/rllib-algorithms.html\n","\n","#set to 0 because of: https://discuss.ray.io/t/scaling-curiosity-like-exploration-modules-on-multiple-workers/2267\n","config[\"num_workers\"] = 0 \n","\n","config[\"exploration_config\"] = {\n","    \"type\": \"Curiosity\", \n","    \"eta\": 0.01, #0.001, \n","    \"lr\": 0.001, \n","    \"feature_dim\": 288, \n","    \"feature_net_config\": {\n","        \"fcnet_hiddens\": [],\n","        \"fcnet_activation\": \"relu\",\n","    },\n","    \"inverse_net_hiddens\": [256],  \n","    \"inverse_net_activation\": \"relu\", \n","    \"forward_net_hiddens\": [256],  \n","    \"forward_net_activation\": \"relu\",  \n","    \"beta\": 0.2,  \n","    \"sub_exploration\": {\n","        \"type\": \"StochasticSampling\",       \n","    }\n","}\n","\n","\n","config[\"vf_clip_param\"] = 600\n","\n","\n","agent = ppo.PPOTrainer(config, env=select_env)\n","\n","print(\"created agent\")"],"metadata":{"id":"akv_RLiCJPT3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# examine the trained policy\n","policy = agent.get_policy()\n","model = policy.model\n","\n","\n","#create environment \n","env = gym.make('VizDoomVeryDenseReward-v0')"],"metadata":{"id":"nfP_rCPxDCtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#restore agent from checkpoint \n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/icm/no_reward\" #_no_tpenalty\"\n","chkpt_dir = 'checkpoint_000300'\n","\n","chkpt_file = chkpt_root + '/' + chkpt_dir \n","print(chkpt_file)\n"],"metadata":{"id":"SRIZBVtIDpWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent.restore(chkpt_file)"],"metadata":{"id":"GutCrOevMFOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = env.reset()\n","sum_reward = 0\n","n_step = 200\n","path = [] \n","actions = [] \n","action_string = ['left', 'right', 'forward']\n","\n","for step in range(n_step):\n","    action = agent.compute_action(state)\n","    actions.append(action)\n","    state, reward, done, info = env.step(action)\n","    sum_reward += reward\n","    \n","    if len(info) > 0:\n","      path.append((info[\"X\"], info[\"Y\"]))\n","\n","    #env.render()\n","\n","    if done == 1:\n","        # report at the end of each episode\n","        print(\"cumulative reward\", sum_reward)\n","        print(f\"total steps: {step}\")\n","        state = env.reset()\n","        sum_reward = 0\n","        break\n","\n","if done == 0:\n","  print(\"cumulative reward\", sum_reward)\n","  print(f\"total steps: {step}\")\n","\n","print(f\"actions:\", [action_string[x] for x in actions])\n","\n","\n","import src.helper_fuctions as helper\n","\n","sectors, health_pos, armor_pos = helper.get_env_layout(config = \"custom/very_dense_reward.cfg\",\n","                                                       scenario = \"custom/train/no_reward_rs.wad\" )\n","helper.plot_layout(sectors, health_pos, armor_pos, path)"],"metadata":{"id":"5Dzc3N9TvIcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# average over 10 episodes\n","n = 20\n","total_reward = 0\n","\n","for i in range(n):\n","\n","      state = env.reset()\n","      sum_reward = 0\n","      n_step = 200\n","      path = [] \n","      actions = [] \n","      action_string = ['left', 'right', 'forward']\n","\n","      for step in range(n_step):\n","          action = agent.compute_action(state)\n","          actions.append(action)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","          \n","          if len(info) > 0:\n","            path.append((info[\"X\"], info[\"Y\"]))\n","\n","          #env.render()\n","\n","          if done == 1:\n","              # report at the end of each episode\n","              print(\"cumulative reward\", sum_reward)\n","              total_reward += sum_reward \n","              print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","      if done == 0:\n","        total_reward += sum_reward \n","        print(\"cumulative reward\", sum_reward)\n","        print(f\"total steps: {step}\")\n","      \n","#total_reward += sum_reward \n","print(f\"Average Reward: {total_reward/n}\")\n","#print(f\"actions:\", [action_string[x] for x in actions])"],"metadata":{"id":"NrB8_QNUlTtw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# average reward over all checkpoints "],"metadata":{"id":"HBjyZ-LQNs3k"}},{"cell_type":"markdown","source":["# No reward"],"metadata":{"id":"bEfINuCWvgwF"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/icm/no_reward\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10 \n","  total_reward = 0\n","  total_steps = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              total_steps += step\n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","      total_steps += n_step\n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_steps/num_episodes)\n","          \n","print(chkpt_mean_rewards)\n"],"metadata":{"id":"LJ_WEAxdvj8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"MLkv-IU0wjzj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sparse setting"],"metadata":{"id":"5cx9tTzvW_gS"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/icm/sparse\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10 \n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","\n","\n","          \n","chkpt_mean_rewards\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"x6U-Ut98NsS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"WyWXTRrUTSd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoints[(36-13):]"],"metadata":{"id":"jzgyP6eqlGXP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dense Setting"],"metadata":{"id":"JFhkyOXnXEH5"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/icm/dense\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10 \n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","\n","\n","          \n","print(chkpt_mean_rewards)\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"MFELmPcyTpco"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"VghRPFsnUptN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Easy 1 room setting "],"metadata":{"id":"7TgQkORVXIyo"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/icm/easy_dense\"#_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10 \n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","          \n","print(chkpt_mean_rewards)"],"metadata":{"id":"tjZM2xBaXIkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#on model with no time penalty \n","import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"DTEwCNtBY0e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#on model with no time penalty \n","import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"xssbdVAGegRv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize Path taken by agent"],"metadata":{"id":"d9iRKfKbbCtY"}},{"cell_type":"code","source":["import src.helper_fuctions as helper\n","\n","sectors, health_pos, armor_pos = helper.get_env_layout(config = \"custom/very_dense_reward.cfg\",\n","                                                       scenario = \"custom/train/new_sparse_rs.wad\" )\n","helper.plot_layout(sectors, health_pos, armor_pos, path)\n","\n","\n"],"metadata":{"id":"MRrcv4KGOnI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","for sector in sectors:\n","        plt.plot([sector[0], sector[1]], [sector[2], sector[3]], color='black', linewidth=2)\n","\n","for pos in path[7:]:\n","        plt.plot(pos[0], pos[1], color='green', marker='o')\n","\n","plt.show()\n","\n","    "],"metadata":{"id":"eOkNlpTrbR1W"},"execution_count":null,"outputs":[]}]}