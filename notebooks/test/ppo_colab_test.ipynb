{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNK1HIUWJqYn6JoQMmqzH+M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup \n","\n"],"metadata":{"id":"z_6MsycFCZFj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdHKp83oCNYP"},"outputs":[],"source":["%%bash\n","# Install deps from \n","# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n","\n","apt-get update\n","\n","\n","apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n","nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n","libopenal-dev timidity libwildmidi-dev unzip\n","\n","# Boost libraries\n","apt-get install libboost-all-dev"]},{"cell_type":"code","source":["!pip install vizdoom\n","!pip install ray \n","!pip install ray['rllib']\n","!pip install Ipython --upgrade\n"],"metadata":{"id":"WF4YrHzsCdQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os \n","from google.colab import drive\n","import sys\n","\n","#need this to load vizdoom module \n","system_path = '/content/drive/MyDrive/GitHub/INM363-Project'\n","drive.mount('/content/drive')\n","sys.path.append(system_path)\n","\n","system_path = '/content/drive/MyDrive/GitHub/INM363-Project/src' \n","sys.path.append(system_path)\n","\n","#need this to use gpu on ray \n","os.environ['PYTHONPATH'] = '/content/drive/MyDrive/GitHub/INM363-Project' \n","os.environ['PYTHONPATH']"],"metadata":{"id":"RFtsvfimEaWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WldQjKYwGDNi"},"outputs":[],"source":["from src.vizdoom_gym.envs.VizDoomEnv import VizdoomEnv\n","from src.vizdoom_gym.envs.VizDoomEnv_def import VizDoomVeryDenseReward"]},{"cell_type":"code","source":["from ray.tune.registry import register_env\n","import gym\n","import ray\n","import ray.rllib.agents.ppo as ppo\n","import shutil\n","import torch"],"metadata":{"id":"gFV-M26PCh3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device: \", device, \"\\n\")\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"AImUdHmoCmsC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create and restore agent from checkpoint"],"metadata":{"id":"RKT1APBrCpwK"}},{"cell_type":"code","source":["ray.shutdown()\n","print(\"Shutdown ray\")\n","# start Ray -- add `local_mode=True` here for debugging\n","ray.init(ignore_reinit_error=True,  num_cpus =2, num_gpus = 1) #local_mode=True,\n","\n","print(\"Initialized ray\")\n","\n","# register the custom environment\n","select_env = \"VizDoomVeryDenseReward-v0\"\n","\n","register_env(select_env, lambda config: VizDoomVeryDenseReward())\n","\n","print(\"registered environment\")\n"],"metadata":{"id":"nA54sQqEC5FG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configure the environment and create agent\n","config = ppo.DEFAULT_CONFIG.copy()\n","config[\"log_level\"] = \"WARN\"\n","config[\"num_workers\"] = 1\n","config[\"framework\"] = \"torch\"\n","config[\"model\"] = {\"dim\": 42, \n","                   \"grayscale\": True,\n","                   }\n","config[\"num_gpus\"] = 1\n","config[\"preprocessor_pref\"] = \"rllib\"\n","config['batch_mode'] = 'complete_episodes'\n","\n","#changing this for evaluation time \n","config['explore'] = False \n","config['in_evaluation'] = True\n","\n","\n","#config[\"horizon\"] = 50\n","agent = ppo.PPOTrainer(config, env=select_env)\n","\n","print(\"created agent\")"],"metadata":{"id":"Y47Q-1FIC6t6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# examine the trained policy\n","policy = agent.get_policy()\n","model = policy.model\n","\n","\n","#create environment \n","env = gym.make('VizDoomVeryDenseReward-v0')\n","#,\n","#               config_file=\"custom\\\\very_dense_reward.cfg\",\n","#                scenario_file=\"custom/test/easy_dense_reward_rs.wad\") \n","#               scenario_file=\"custom/test/dense_reward_fixed_start.wad\") "],"metadata":{"id":"nfP_rCPxDCtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#restore agent from checkpoint \n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/ppo/easy_no_reward\"\n","chkpt_dir = 'checkpoint_000010'\n","\n","chkpt_file = chkpt_root + '/' + chkpt_dir \n","print(chkpt_file)\n"],"metadata":{"id":"SRIZBVtIDpWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent.restore(chkpt_file)"],"metadata":{"id":"GutCrOevMFOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = env.reset()\n","sum_reward = 0\n","n_step = 200\n","path = [] \n","\n","for step in range(n_step):\n","    action = agent.compute_action(state)\n","    state, reward, done, info = env.step(action)\n","    sum_reward += reward\n","    \n","    if len(info) > 0:\n","      path.append((info[\"X\"], info[\"Y\"]))\n","\n","    #env.render()\n","\n","    if done == 1:\n","        # report at the end of each episode\n","        print(\"cumulative reward\", sum_reward)\n","        print(f\"total steps: {step}\")\n","        state = env.reset()\n","        sum_reward = 0\n","        break\n","\n","if done == 0:\n","  print(\"cumulative reward\", sum_reward)\n","  print(f\"total steps: {step}\")\n"],"metadata":{"id":"5Dzc3N9TvIcM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualize Path taken by agent"],"metadata":{"id":"d9iRKfKbbCtY"}},{"cell_type":"code","source":["import src.helper_fuctions as helper\n","\n","sectors, health_pos, armor_pos = helper.get_env_layout(config = \"custom/very_dense_reward.cfg\",\n","                                                       scenario = \"custom/train/easy_dense_reward_rs.wad\" )\n","helper.plot_layout(sectors, health_pos, armor_pos, path)\n","\n","\n"],"metadata":{"id":"MRrcv4KGOnI0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# average reward over all checkpoints"],"metadata":{"id":"YclGsBlyCcNQ"}},{"cell_type":"markdown","source":["# Sparse Setting"],"metadata":{"id":"SHgiPGe7CO-Z"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/ppo/sparse\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","\n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 1\n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","\n","\n","          \n","print(chkpt_mean_rewards)\n","\n"],"metadata":{"id":"SK3WRtyVCjb8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"eOkNlpTrbR1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dense Setting"],"metadata":{"id":"pgfR_rr2DdXL"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/ppo/dense\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","\n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 1\n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","\n","\n","          \n","print(chkpt_mean_rewards)\n","\n"],"metadata":{"id":"mAuIRwPNEX7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"-Pz7Z2T7EvOD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Easy Dense"],"metadata":{"id":"NsdPqFd4EecY"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/ppo/easy_dense\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","\n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 1\n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","\n","          \n","print(chkpt_mean_rewards)"],"metadata":{"id":"Gz4m173_Egfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"sKN8VXCYHTAo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Easy no reward"],"metadata":{"id":"cNHrT7D02zbv"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/ppo/easy_no_reward\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","\n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 1\n","  total_steps = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_steps += step \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_steps += step \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_steps/num_episodes)\n","\n","          \n","print(chkpt_mean_rewards)"],"metadata":{"id":"aLQ-8I8dHDCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"F6lOQH4P29LZ"},"execution_count":null,"outputs":[]}]}