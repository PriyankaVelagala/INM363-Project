{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNsP88rm+AEGQoxTpDONhcw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"szQR29sp3EnS"}},{"cell_type":"code","source":["%%bash\n","# Install deps from \n","# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n","\n","apt-get update\n","\n","\n","apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n","nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n","libopenal-dev timidity libwildmidi-dev unzip\n","\n","# Boost libraries\n","apt-get install libboost-all-dev"],"metadata":{"id":"1wOwu4mg3EKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxLURmm320dA"},"outputs":[],"source":["!pip install vizdoom\n","!pip install ray \n","!pip install ray['rllib']\n","!pip install Ipython --upgrade\n"]},{"cell_type":"code","source":["import os \n","from google.colab import drive\n","import sys\n","\n","\n","#need this to load vizdoom module \n","system_path = '/content/drive/MyDrive/GitHub/INM363-Project'\n","drive.mount('/content/drive')\n","sys.path.append(system_path)\n","\n","system_path = '/content/drive/MyDrive/GitHub/INM363-Project/src' \n","sys.path.append(system_path)\n","\n","#need this to use gpu on ray \n","os.environ['PYTHONPATH'] = '/content/drive/MyDrive/GitHub/INM363-Project' \n","os.environ['PYTHONPATH']"],"metadata":{"id":"qRd7eIwy3lMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from src.vizdoom_gym.envs.VizDoomEnv import VizdoomEnv\n","from src.vizdoom_gym.envs.VizDoomEnv_def import VizDoomVeryDenseReward"],"metadata":{"id":"VS1T6M3r3pWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ray.tune.registry import register_env\n","import gym\n","import os\n","import ray\n","import ray.rllib.agents.ppo as ppo\n","from ray.rllib.algorithms.callbacks import RE3UpdateCallbacks\n","import shutil\n","import torch"],"metadata":{"id":"9QBkzwii3rK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device: \", device, \"\\n\")\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"wHlypTqP3uYp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create and restore agent from checkpoint\n"],"metadata":{"id":"ZvFWuCx1CUVI"}},{"cell_type":"markdown","source":["# Initialize Ray"],"metadata":{"id":"ISS2wOf93yzg"}},{"cell_type":"code","source":["\n","ray.shutdown()\n","print(\"Shutdown ray\")\n","# start Ray -- add `local_mode=True` here for debugging\n","ray.init(ignore_reinit_error=True,  num_cpus =2, num_gpus = 1) #local_mode=True,\n","\n","print(\"Initialized ray\")\n","\n","# register the custom environment\n","select_env = \"VizDoomVeryDenseReward-v0\"\n","\n","register_env(select_env, lambda config: VizDoomVeryDenseReward())\n","\n","print(\"registered environment\")\n"],"metadata":{"id":"Okj24G46Cbnu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Trainer config"],"metadata":{"id":"zV8H12_g4KJZ"}},{"cell_type":"code","source":["# configure the environment and create agent\n","config = ppo.DEFAULT_CONFIG.copy()\n","config[\"log_level\"] = \"WARN\"\n","config[\"model\"] = {\"dim\": 42, \n","                   \"grayscale\": True,\n","                   }\n","config[\"num_gpus\"] = 1\n","config[\"preprocessor_pref\"] = \"rllib\"\n","#changing this for evaluation time \n","config['explore'] = True #for ICM and RE3 might need to be left true to use the exploration module \n","config['in_evaluation'] = True\n"],"metadata":{"id":"aF2dBoKZ3-cv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RE3Callbacks(RE3UpdateCallbacks, config[\"callbacks\"]):\n","  pass\n"],"metadata":{"id":"1HjQVsdSbuiT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config[\"framework\"] = \"tf\"\n","\n","\n","\n","#https://github.com/ray-project/ray/blob/c9c3f0745a9291a4de0872bdfa69e4ffdfac3657/rllib/utils/exploration/tests/test_random_encoder.py#L35\n","\n","\"\"\"\n","config[\"seed\"] = 123\n","config[\"callbacks\"] = RE3Callbacks\n","config[\"exploration_config\"] = {\n","            \"type\": \"RE3\",\n","            \"embeds_dim\": 128,\n","            \"beta_schedule\": \"constant\",\n","            \"sub_exploration\": {\n","                \"type\": \"StochasticSampling\",\n","            }\n","        }\n","\"\"\"\n","\n","config[\"seed\"] = 12345\n","config[\"callbacks\"] = RE3Callbacks\n","config[\"exploration_config\"] = {\n","    \"type\": \"RE3\",\n","     \"embeds_dim\": 128,\n","     \"rho\": 0.1, \n","     \"k_nn\": 7, \n","     \"encoder_net_config\": {\n","         \"fcnet_hiddens\": [],\n","         \"fcnet_activation\": \"relu\",\n","     },\n","     \"beta\": 0.2,\n","     \"beta_schedule\": 'constant',\n","     \"sub_exploration\": {\n","         \"type\": \"StochasticSampling\",\n","     }\n","}\n","\n","agent = ppo.PPOTrainer(config, env=select_env)\n","\n","print(\"created agent\")"],"metadata":{"id":"qsf9x6OO6CTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#create environment \n","env = gym.make('VizDoomVeryDenseReward-v0')\n"],"metadata":{"id":"lNtDNMy-Cyzb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#restore agent from checkpoint \n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/re3/no_reward\" #_no_tpenalty\"\n","chkpt_dir = 'checkpoint_000100'\n","\n","chkpt_file = chkpt_root + '/' + chkpt_dir \n","print(chkpt_file)\n"],"metadata":{"id":"N_xI-T-6C1FL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent.restore(chkpt_file)"],"metadata":{"id":"JKhIHrOYC5Iu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = env.reset()\n","sum_reward = 0\n","n_step = 500#200\n","path = [] \n","actions = [] \n","action_string = ['left', 'right', 'forward']\n","\n","for step in range(n_step):\n","    action = agent.compute_action(state)\n","    actions.append(action)\n","    state, reward, done, info = env.step(action)\n","    sum_reward += reward\n","    \n","    if len(info) > 0:\n","      path.append((info[\"X\"], info[\"Y\"]))\n","\n","    #env.render()\n","\n","    if done == 1:\n","        # report at the end of each episode\n","        print(\"cumulative reward\", sum_reward)\n","        print(f\"total steps: {step}\")\n","        state = env.reset()\n","        sum_reward = 0\n","        break\n","\n","if done == 0:\n","  print(\"cumulative reward\", sum_reward)\n","  print(f\"total steps: {step}\")\n","\n","print(f\"actions:\", [action_string[x] for x in actions])\n","\n","\n","import src.helper_fuctions as helper\n","\n","sectors, health_pos, armor_pos = helper.get_env_layout(config = \"custom/very_dense_reward.cfg\",\n","                                                       scenario = \"custom/train/no_reward_rs.wad\" )\n","helper.plot_layout(sectors, health_pos, armor_pos, path)"],"metadata":{"id":"0-rlZ7ldC7qn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# average reward over all checkpoints "],"metadata":{"id":"e8WXepcgFi_F"}},{"cell_type":"markdown","source":["# Sparse"],"metadata":{"id":"uiH5u5-2QBtY"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/re3/sparse\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10 \n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","          \n","print(chkpt_mean_rewards)\n"],"metadata":{"id":"aw0oAvkrQFI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"PxXsDM8MQKFO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dense"],"metadata":{"id":"j4eTgBJ9UnMF"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/re3/dense\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10 \n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","          \n","print(chkpt_mean_rewards)\n"],"metadata":{"id":"JSoPqWQfUnia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"8_85UeRRU0Vu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# easy dense "],"metadata":{"id":"lULCUxvWFlJn"}},{"cell_type":"code","source":["from fnmatch import fnmatch \n","\n","chkpt_root = \"/content/drive/MyDrive/GitHub/INM363-Project/model_checkpoints/re3/easy_dense\" #_no_tpenalty\"\n","\n","pattern = '*checkpoint*'\n","checkpoints = [x for x in os.listdir(chkpt_root) if fnmatch(x,pattern )]\n","\n","print(\"Total number of checkpoints: \", len(checkpoints))\n","\n","chkpt_mean_rewards = [] \n","\n","for chkpt_dir in checkpoints:\n","  chkpt_pth = chkpt_root + '/' + chkpt_dir \n","  print(chkpt_dir)\n","  agent.restore(chkpt_pth)\n","\n","  num_episodes = 10 \n","  total_reward = 0\n","  #chkpt_mean_reward = 0 \n","\n","  for i in range(num_episodes):\n","    state = env.reset()\n","    sum_reward = 0\n","    n_step = 200\n","\n","    for step in range(n_step):\n","          action = agent.compute_action(state)\n","          state, reward, done, info = env.step(action)\n","          sum_reward += reward\n","\n","          if done == 1:\n","              total_reward += sum_reward \n","              #print(f\"total steps: {step}\")\n","              state = env.reset()\n","              sum_reward = 0\n","              break\n","\n","    if done == 0:\n","      total_reward += sum_reward \n","\n","    #print(total_reward)\n","\n","  chkpt_mean_rewards.append(total_reward/num_episodes)\n","\n","\n","          \n","chkpt_mean_rewards\n","\n"],"metadata":{"id":"m-jlgh9qE6Kh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(chkpt_mean_rewards)\n","plt.show()"],"metadata":{"id":"95znwJtuJu5j"},"execution_count":null,"outputs":[]}]}